# Length Generalization of Small Transformers

A fork of the nanoGPT repo, used to study the length generalization of small transformers for my CS330 final project. It contains the code for three parts of the project:
1. Single-task baselines from [this paper](https://arxiv.org/pdf/2310.16028.pdf), and two variants of the multi-task case that we tried out.
2. Studying length generalization of modular arithmetic mod 5 (+, -, x), using the scratchpad + index hints technique developed for parity in the paper above.
3. In-Context Learning with the same technique to understand its effectiveness.

The link to the full write-up can be found [here](https://github.com/ramvenkat98/nanoGPT-length-generalization/blob/master/cs330_final_report.pdf).
